# Notes
This are my notes from the class: Hadoop Platform and Application Framework.

Part of: https://www.coursera.org/specializations/big-data

# Hadoop
Apache Hadoop is an open source software framework for storage and large
scale processing of data-sets on clusters of commodity hardware.

- Hadoop moves computation to data.
- Hardware failures handled automatically
- Hadoop mapReduce/HDFS originally derived from Google's mapReduce and FS.
- Keep all data - schema and read style: Read the data and create the schema as
  reading happens.

## Basic modules
- Hadoop common.
 - Libraries and utilities by other Hadoop modules
- Hadoop Distributed File System (HDFS)
 - Distributed file system that stores data in commodity machine, providing
   very high aggregate badnwidth across the entire cluster.
- Hadoop MapReduce
 - Programming model that scales data acorss a lot of different processes.
- Hadoop YARN
 - Resource management platform responsible for managing compute resources
   in the cluster and using them in order to schedule users and apps.

## Hadoop Distributed File System
Distributed, scalable, and portable file system written in Java in order to
support the Hadoop framework.

- Each node in Hadoop instance typically has a single name node.
- And a cluster of data nodes that formed this HDFS cluster
- Each HDFS stores large files, tipicallt in ranges of Gb, Tb
- Reliability is achieved by replicating across multiple hosts

- secondary name node, builds a snapshot of the primary, remembers which
  systems saves to the local and remote directories.

The typical MapReduce engine will consist of:
- a job tracker, to which applications can submit MapReduce jobs, and this job
  tracker typically pushes work to all the available
- task trackers, trying to keep the work as close as possible to the data as
  balanced as possible.

### YARN
Separates the resource management and the processes component. Splits up
- job tracker
- resource management
- the job scheduling and monitoring into 2 separate units.

Enhances the power of a Hadoop compute cluster, without being limited by the
mapReduce framework.
- Focuses exclusively on scheduling
- improves cluster utilization, according.
 - capacity
 - guarantees
 - fairness
 - SLAs
- Supports other workloads
 - Graphs process
 - Uterative modeling
 - Machine learning

## Hadoop Zoo ?
Cloudera Stack. Looks like around Hadoop there are a series of technologies
that allow the interaction with Hadoop, different stacks exists developed (or
architected) by different companies (Google, Yahoo, Facebook, LinkedIn) and the one
stack that we will use in this class is the Cloudera VM.

## Hadoop Ecosystem Major Components

### Apache Sqoop
- Sqoop: SQL to Hadoop
- Command line tool.
- Let us import individual tables or entires DBs into our HDFS
- Generates Java classes to interact and import data.
- Tool designed for efficiently transferring bulk data between Hadoop and
  structured datastores such as relational DBs.
- You setup import jobs in Sqoop

### HBase
- Its design catters to application that require really fast random access to
  significant data set.
- Column oriented DB management system
- Key-value store
- Based on Google Big Table
- Can hold extremely large data
- Dynamic data model
- Not a Relational DBMS

### PIG
- A scripting language to create MapReduce programs using Hadoop
- High level programming on top of Hadoop MapReduce
- Pig Latin
- Excels at describing data analysis problems as data flows
- You can do all the data manipulation within HAdoop just by usgin pig.
- Data analysis problems as data flows
- Originally developed at Yahoo in 2006
- In addition to the user defined functions, facilities in pig allow you to
  have code in many different languages (JRuby, JPython, Java)
- You can execute PIG scripts in other languages
- PIG is a component to buid much larger more complex application that can
  tackle real business problems.

ETL -> Extract, transform, load

ETL transaction model: describes how a process will extract data from a source,
transporting according to the rule set that we specify, and then loads it into
a data store.

PIG can ingest data from files, streams or other by using the users defined
functions (UDF) that we can write ourselves.

Once all the data is availablre it can select, iterate and all sort of
transformation, the same UDF allow us to pass the data to more complex
algorithms for more complex transformation, take that data and store it in the
Hadoop file system.

### Apache Hive
- Data warehouse software the facilitates querying and managing large datasets
  residing in distributed storage.
- Provides a mechanism to project structure on top of all this data and allow
  us to user SQL like queries to access the data.
- The query language is called Hive QL.
- Allows to write MapReduce programs to park inside the custom mappers and
  reducers when it is inconvenient, too complex or inneficient to express the
  logic we would like to express in processing our data within the Hive
  language.

### Oozie
- Workflow scheduler system to manage Apache Hadoop job
- Oozie workflow jobs are called DAGs or Directed Graphs.
- Oozie coordinator jobs: recurrent oozie workflow jobs that are triggered by
  frequency or data availability.
- Supports: MapReduce, Pig, Apache Hive, Sqoop, etc.

### Zookeeper
- Provides operational services for a Hadoop cluster
- Provides a distributed configuration service (from the professor talking)
- Provides a naming registry for the entire distributed system.
- Centralized service for maintaining configuration information, naming,
  providing distributed synchronization (from the slides)
- Distributed applications use zookeeper to store immediate updates to
  important configuration information on the cluster itself.

### Flume
- Distributed, reliable and available service for efficiently collecting,
  aggregating and moving large amounts of log data.
- Its architecture is based on streaming data flows.
- Robust, fault tolerant, tunable

### Impala
- Cloudera impala is CLoudera's open source massively parallel processing (MPP)
  SQL query engine for data stored in a computer cluster running Apache Hadoop.
- Brings scalable parallel DB technology to Hadoop
- Allow users to submit low latency queries to the data that's stored within
  HDFS or HBase without a ton of data movementn and manipulation.
- Integrated with Hadoop and works within the same power system, format
  metadata, security and reliability resources and management workflows.
- Allows us to submit SQL like queries at much faster speed with a lot less
  latency.

### Spark
- One of the alternatives over the traditional Hadoop platform
- A scalable data analytics platform that incorporates primitives for in-memory
  computing.
- Allows us to exercise some different performance advantages over traditional
  Hadoop's cluster storage system approach
- Implemented and supports Scala
- Good for more complex kinds of analytics
- Great at supporting machine learning libraries

In contrast to Hadoop's two stage disk base map reduce paradigm, Spark is
a multistage in memory primitive that provides up to a 100 times faster
performance in some applications.

By allowing users to load data into cluster memory and querying repeatedly,
Spark is really well suited for ML applications that often time have iterative
sorting in memory type of computation.

Spark requires a cluster management and a distributed storage System.

For cluster management, Spark supports standalone native Spark clusters, or run
Spark on top og Hadoop YARN, or Apache MESOS.

For distributed storage, Spark can interface with any of the variety of storage
system, including HDFS, Amazon S3, or custom.
