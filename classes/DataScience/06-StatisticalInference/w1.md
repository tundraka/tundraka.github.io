#Introduction
Statistical inference: Go beyond the data, trying to generate new knowledge.
Trying to go beyond our data to a population to get answers. It's the only
formal inference system that we have.

> The process of generating conclusions about a population from a noisy sample.

With statistical inference, the estimators have actual things that they're
estimating. (What are estimators?)

Many different modes of statistical inference.
- bayesians
- frequentists

Frequency style thinking: think of probability like how we think of gambling.
The idea that we can repeat an experiment over and over and the percentage of
times that something happens defines that population parameter.

- Causation
- Association

#Probability
Given a random experiment (rolling a die) a probability measure is a population
quantity that summarizes the randomness
- random experiment: ?
- P = pq summarizes R
 - P = Probability measure
 - pq = population quantity
 - R = randomness

In the case of the rolling die experiment: population is an intrinsic property of
the die, not of something that is a function of a simple set of fixed rolls of
the die.

- Intrinsic: of or relating to the essential nature of a thing, inherent.

When we talk about probability is not something that is in the data that we have,
but as a conceptual thing that exists in the population that we would like to
estimate.

**Probability calculus**: rules that probability has to follow. Forms the
foundation for all thinking on probability.

Probability operates on the potential outcomes from an experiment. For
example:
- When you roll a die you could get a 1.
- Or the result can be in the set {1, 2}
- Or the roll was an even/odd number
- etc.

Probability is a function that takes any of these sets of possible outcomes and
assigns it a number between 0 and 1.

We have to have the rule that the probability that something occurs, you roll
the die and you get a number, must be 1.

The probability of the union of any two sets of outcomes that have nothing in
common (mutually exclusive) is the sum of their respective probabilities.

For example one possible outcome is that we get A={1, 2} and another outcome is
that we get B={3, 4} those two outcomes are mutually exclusive, they cannot
simultaneously occur. The probability of the union, that I get the outcome of A
or B is the sum of the probability of the A outcome plus the probability of the
B outcome `P(AUB) = P(A) + P(B)`.

These simple rules are all that is necessary to generalize to all the rules
that we think that probability should follow. Discovered by the Russian
mathematician Kolmogorov.

####Rules probability must follow
- The probability that nothing occurs is 0
- The probability that something occurs is 1
- The probability of something is 1 minus the probability that the opposite
  occurs. In the context of the die, the probability of getting an even number
  is 1 minus the probability of getting an odd number.
- The probability of at lest one of two (or more) things that are mutually
  exclusive is the sum of their respective probabilities
- If an event A implies the occurrence of event B, then the probability of
  A occurring is less than the probability that B occurs. We can think of
  A being a subset of B.
- For any two events the probability that at least one occurs is the sum of
  their probabilities minus their intersection. `P(AUB) = P(A) +
  P(B) - P(A^B)`. You cannot just add probabilities if they have a non trivial
  intersection.

#Probability mass functions
Probability calculus is useful to understand the rules that probabilities must
follow. However, we need ways to model an think about probabilities for numeric
outcomes of experiments (broadly defined). Something that is easier to work
with for numeric outcomes of experiments.

Densities and mass functions for random variables are the best starting point
for this. Probably the most famous example of a density is the so called bell
curve. We would be able to say what it means to say that the data follow a bell
curve.

When we talk about probabilities associated with the bell curve or the normal
distribution, we are talking about population quantities. It's not about what
occurs in the data. We are going with this is:use the data to estimate
properties of the population. Before we start working with the data, we need to
develop our intuition for the population quantities work

####Random variables
A **random variable** is a numerical outcome of an experiment. The random
variables that we will study come in **discrete** and **continuous**

- discrete: you can count, e.g.: number of web hits, outcome of a die. And even
  things that are not numeric like colors which can be associated with
  a number, red:1, blue:2, etc. We are going to assign a probability to every
  value that they can take
- continuous: can take any value in a continuum, we will assign probabilities
  to ranges that they can take.


####Examples of random variables
- The outcome of flipping a coin (0-1)
- The outcome of rolling a die
- Website traffic on a given day, treat as discrete, no higher bound on it. We
  would probably use the poison distribution to model that.
- BMI of a subject 4 years after a baseline measurement. A continuous random
  variable.
- Hypertension status of a subject randomly drawn from a population. We might
  give 1=hypertension, 0=no hypertension, modeled as discrete.
- Number of people who clicked on an add, similar to the website traffic.
- Intelligence quotients, normally modeled as continuous.

####Probability Mass Function (PMF)
Discrete random variables will be assigned a probability for each value they can
take. This is what the PMF will do, this function takes any value that
a discrete random variable can take and assigns the probability that it takes.

The PMF for a die roll will take 1/6 for each value that the die can produce.

There are rules that the PMF must satisfy in order for it to satisfy the basic
probability rules that were outlined before
1. it must be always be larger than 0
2. The sum of the possible values that the random variable can take has to add
  up to 1.

In the case of the die, for 2. to met, we can add the probabilities of each of
the probabilities that it can generate

```
X=random variable that is the result of a die roll
P=PMF associated with X
1 = P(1) + P(2) + P(3) + P(4) + P(5) + P(6)
1 = 1/6  + 1/6  + 1/6  + 1/6  + 1/6  + 1/6
```

PMFs that are useful:
- binomial: the canonical one for modeling the flip of a coin
- poisson: the canonical one for modeling counts

Most famous example of a PMF, the result of a coin flip, the so-called
Bernoulli distribution. This is for a fair coin:

```
X=the result of a coin flop, 0=tail, 1=head. An upper case letter represents
a potentially unrealized value of the random variable.
p(x) = (1/2)^x(1/2)^(1-x) for x=0,1

p(0) = (1/2)^0(1/2)^(1-0) = 1 * (1/2)^1 = 1/2
p(1) = (1/2)^1(1/2)^(1-1) = (1/2)^1 * 1 =  1/2
```

What about an unfair coin
```
ø=theta
p(x) = ø^x(1-ø)^(1-x) for x=0,1
p(0) = ø^0(1-ø)^(1-0) = 1 - ø
p(1) = ø^1(1-ø)^(1-1) = ø
```

This is useful for modeling the prevalence of something. For example, if we
want to model the prevalence of hypertension, we might assume that the
population or the sample that we're getting isn't unlike flips of biased coin
with the success probability theta. The issue is that we don't know theta, so
we will use our data to estimate this population proportion.

#Probability density functions (PDF)
Associated with continuous random variables. These are the functions that it
has to follow.
- its ouput must be ≥ 0 everywhere
- the total area under it must be 1

Areas under a PDF correspond to probabilities for that random variable.

If we say, intelligence quotients are normally distributed with a mean of 100
and a standard deviation of 15. This implies that the population follows
a specific bell shaped looking curve, and if I were to draw a sample, the
probability that a person from that sample has an IQ between 100-115 will be
the area between those two segments.

The PDF is a statement about the population of IQs in this case. Not
a statement about the data itself, we will use the data to evaluate that
assumption and to evaluate statements about the population probability, when we
say probability we are talking about a population quantity.

One thing to note is that if we want to know the probability of a single
value, it will be 0, because the are of a line is 0. This is a quirk from
modeling random variables as if they have infinity precision.

Instead of working with a Bell shapped curve, we will use another. The
following code can be found in the [courses git repo][1].

```
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(   0, 0, 2, 0, 0  )
plot(x, y, lwd=3, frame=F, type='l')
```

This function meets the 2 rules stated before
1. Its output should be ≥0: it is since the ranges are very clear
2. It's area should be 1: a = bh/2 = 1\*2/2 = 1

What's the probability that 75% of fewer calls get addressed? Turns out this is
another right triangle, in this case a = bh/2 = 0.75 \* 1.5 / 2 = 0.562 = 56%.
It turns out that this density is a know case of the beta distribution. In this
case we don't need it since we are working with triangles, but more
complicated setting may require it.

```R
pbeta(0.75, 2, 1) # p=probability
```

###CDF and survival function
Certain areas of the density are useful that we give them names. For example,
the *cumulative distribution function* (CDF) of a random variable X, returns the
probability that a random variable is less than or equal to the value x. This
applies to whether x is discrete or continuous.

```
F(x) = P(X ≤ x)
```

In the beta distribution that we looked at. the `pbeta` function in R always
returns whatever the first argument is the probability of being less than or
equal to that.

```
F(.75) # the area under the graph
pbeta(.75, 2, 1) # R equivalent for this example.
```

`p(density name)` in R (e.g. `pbeta`) it's actually just returning the
*cumulative distribution function*.

####Survival function
The **survival function** of a random variable X is defined as the probability
that the random variable is greater that the value x.

```
S(x) = P(X > x)
#Notice that
S(x) = 1 - F(X)
```

Example: What are the survival function and CDF from the density considered
before? For 1≥x≥0. We want to know what's the probability that 40%, 50%, 60% or
fewer of the calls get answered in a given day.

```
F(X) = P(X ≤ x) = bh/2 = x * 2x / 2 = x * x = x^2
```

Will give the probaility of that percent calls or fewer get answered on
a certain day.

```R
pbeta(c(0.4, 0.5, 0.6), 2, 1)
#[1] 0.16 0.25 0.36
```

###Quantiles
Sample quantiles: if you were the 95th percentile on an exam, you know that 95%
of the people scored worse than you and 5% scored better. These are the *sample
quantiles*.

In the sample quantile, if you want to define the *95th percentile* or *0.95
quantile* what you'll do is line up the observations from least to greatest, and
you'll find the grade or point such that 95% of the observations lie below it.

The αth quantile of a distribution with distribution function F is the point
x\_α so that F(x\_α) = α.

Drawing a density, the distribution function evaluated at x, is the area below
the point x, which is the probability that a random variable from this
population is less than or equal to x. We may think of it as a population of
test scores.

As mentioned the *distribution function* will 'highlight' an area of the density
that is below x, this area represents the probability of getting a score for
a randomly drawn student from this population of x or lower.

The alpath quantile is we move the limit that x represents until we find the
point x\_α, so that exactly α probability lies below it

- A **percentile** is simple a quantile with  α expressed as a percent
- The **median** is the 50th percentile.

Reinforcing. The 95th percentile of a *population distribution* is the point
such that the probability a random variable is drawn from that population is
less than 95%. Meaning the 'highlighted' area will contain about 95% of the
observations that we draw from this population will be less that .95; and the
probability that a random variable drawn from that population is more is 5%

Example: what's the median of the distribution that we were working before?

```
F(x) = x^2 = 0.5
sqrt(0.5) = 0.7071
```

This means that on about 50% of the days, 70% of the phone calls or fewer get
answered, and on about 50% of the days, about 70% of the phone calls or more
get answered, mmmh I think the professor meant to say 30%?

We almost never go through the process of working directly with the densities
to calculate quantiles, because the distribution we work with are common and
this is already been done for us.

In R, a q in front of a function density name gives the quantile, for example:
`qbeta`.

```R
qbeta(0.5, 2, 1)
# 0.7071
```

###Summary
- you might be wondering at this point "I've heard of the median, it didn't
  require integration, where's the data?"
- We're referring to are **population quantities**. Therefor, the median being
  discussed is the **population median**.
- A probability model connects the data to the population using assumptions.
- Therefore the median we're discussion is the **estimand**, the sample median
  will be the **estimator**.

This is the formal process of statistical inference. Linking your sample to a
population.

#Resources
- Brian's class Mathematical Biostatistics Boot Camp 1 has many relevant
  lectures at a more advanced level
  https://www.youtube.com/watch?v=jkUqDVtpKs4&list=PLpl-gQkQivXhk6qSyiNj51qamjAtZISJ-

[1]: https://github.com/bcaffo/courses/tree/master/06_StatisticalInference/02_Probability
