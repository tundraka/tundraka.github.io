#Variance
Population mean, where the distribution is center at. How fat or thin, or
rather how spread out or how concentrated the density is around the mean is the
*variance*

The variance of a random variable is a measure of spread. If `X` is a random
variable that has mean `µ`, the variance is exactly the expected square
distance the random variable is from the mean. There's also a short version,
the expected value of `X` squared minus the expected value of `X` quantity
squared.

```
Vax(X) = E[(X-µ)^2] = E[X^2] - E[X]^2
```

Densities with higher variance are more spread out than densities with lower
variance and the square root of the variance is called the *standard
deviation*. The variance is expressed in the unit squared, while the std dev is
expressed as the same units as `X`.

In the example of tossing a die we have

```
E[X] = 3.5
# the formula to get the expected value of X^2
E[X^2] = 1^2 * 1/6 + 2^2 * 1/6 + ... + 6^2 * 1/6
E[X^2] = 15.17
Var(X) = E[X^2] - E[X]^2 ≈ 2.92
```

What's the variance from the result of the toss of a coin with probability of
heads (1) of `p`?

The population variance associated with the distribution given by the flip of
a coin a biased coin is exactly `p(1-p)`

```
E[X] = 0 * (1 - p) + 1 * p = p
E[X^2] = E[X] = p
Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1-p)
```

population densities as the variance changes.

Just like the population mean and the sample mean were directly analogous, the
population variance and the sample variance are directly analogous. The
population mean was the center of mass of the population. The sample mean was
the center of mass of the observed data. The population variance is the
expected square of distance of a random variable from the population around the
population mean.

The sample variance is the average square of distance of the observed
observations minus the sample mean.

```
X^_ = sample mean. This is expressed as a capitol X with a underscore on top of
it.
S^2 = ∑_(i-1)(X_i - X^_)^2 / (n-1)
```

The variance of a sample variance. The sample variance is exactly a function of
the data. So, it's also a random variable, thus it also has a population
distribution. That distribution has a expected value and that expected value is
the population variance that the sample variance is trying to estimate. As we
collect more and more data, the distribution of the sample variance is going to
get more concentrated around the population variance it's trying to estimate.
The square root of the sample variance is the sample.

Ok, this is an example, that I don't understand... Simulte ten standard normals
and take their sample variance and do that over and over again. Then I get
a distribution of sample variance. Looks like there was a mention of Rissotto,
not sure if the professor is refering to the delicious italian mix of goodies
in rice. I used to eat a lot of rissotto (with mayo... If I think I'm fat now,
you should see the photos from then (before going to do my master), I got rid
of a lot of stuff). If we were to sample enough of them, its center of mass
will be exactly 1. The variance from the original population that I was
sampling, the std normal which has variance. More data will yield a more
concentrated estimate around what the sample variance is trying to estimate.
What this means is that the variance is a good estimate of the population
variance. More data means, the distribution of the sample variance gets more
concentrated about what it's trying to estimate. ANd that it's centered in the
right place, that it's unbiased. This unbiased is why we divide by `(n-1)` instead
of just `n`. That makes it unbiased.

#Standard error of the mean.
Mean: the average of random sample from a population is itself a random
variable, it has its own population mean and population variance (1). The
population mean is the same as the original population. We have a result that
relates its variance back to the variance of the original population (2). The
variance of the sample mean decreases to zero as it accumulates more data. As
we already know, the mean becomes more concentrated about the population mean
that we are trying to eestimate as we collect more data.

We only get one sample mean in a given data set, we don't get lots of repeated
versions to investigate its variability the way we do with this fabricated
simulation experiments.

We can estimate `σ^2`, and we know `n`, so we know quite a bit about the
distribution of the sample mean from the data we observed.

The sqare root fo the statistic is sigma over the square root `n` is the
standard error of the mean and we use thi notation, we call a standard
deviation of the distribution of a statistic, we call it a standard error (2).
We talk about the std error of a mean to imply a number that represent
variability of means, and the std error of a regression coefficient talks about
the variability in regression coefficients...

I'm just typing everything verbatim. I'm not even trying to understand what's
going on in the video lectures...

```
(1) E[X^_] = µ
(2) Var(X^_) = σ^2/n
```
