#tl;dr
- people are busy
- Results from analyses are sometimes presented in oral form, but often the
  final cut is presented via email.
- Good idea to break the results in different levels of granularity/detail
- Getting responses from busy people: http://goo.gl/sJDb9V

#Hierarchy of information: Research paper.
- Title/Author list
- Abstract
- Body/results
- Supplementary materials/ the gory details
- Code / Data / really gory detail.s

#Hierarchy of information: Email presentation.
- Subject line / sender info 
 - At a minimum; include on
 - Can you summarize findings in one sentence?
- Email body
 - A brief description of the problem / context; recall what was proposed and
   executed summarize findings/results; 1-2 paragraphs.
 - If action needs to be taken as a result of this presentation, sugges some
   options and make them as concrete as possible
 - If questions need to be addressed, try to make them yes/no
- Attachments
 - R markdown
 - knitr report
 - stay concise.
- Links to supplementary materials
 - Code/software/data
 - github repository/project website.

#Reproducible research checklist

####Do: Start with good science.
- GIGO
- Coherent, focused question simplifies many problems
- Working with good collaborators reinforces good practices
- Something that's interesting to you will motivate good habits.

####Don't: Do things by hand
- Editing spreasheets of data to "clean it up"
 - Removing outliers
 - QA/QC
 - Validating
- Editing tables or figures (e.g. rounding, formatting)
- Downloading data from a web site (clicking links in a web browser)
- Moving data around your computerm spliting/reformating data files
- "we're just doing this once..."

Things done by hand need to be clearly documented.

####Don't: point and click
- many data processing/statistical analysis packages have graphical user
  interfaces (GUIs)
- GUIs are convenient/intuitive but the actions you take with GUI can be
  difficult for others to reproduce
- Some GUIs procude a log file or script which includes equivalent commands;
  there can be saved for later examination
- In general, be careful with data analysis software that is highly
  interactive; easy of use can sometimes lead to non-reproducible analyses.
- Other interactive software, such as text editors, are usually fine.

####Do: teach a computer
- For any step part of your analysis try to teach a computer to do it.
- Telling a computer what to do requires to write down what exactly needs to be
  done.
- Teaching a computer almost guarantees reproducibility

For example, by hand, you can
1. Go to UCI Machine learning repo at http.....
2. DOwnload the X data set by clicking link x, download in y directory, unzip
   it.open, set the names, etc.

####Do: use some version control
- show things down
- Add changes in small chunks (not just one single commit with everything)
- Tracl/tag snapshots; revert to old versions
- Software like github.bitbucket/sourceforge make it easy to publis results.

####Do: keep track of your software environment
- It can be difficult for reproducibility if you work with different tools.
- Computer architecture
- OS
- Software toolchain: compilers, interpreters, command shell, programming
  language (C, Perl, Python, etc), DBs, data analysis software
- Supporting software/infraestucture: Libraries, R packages, dependencies
- External dependencies: web sites, data repos, remote DBs, SW repos.
- Version numbers: ideally for everything.

in R you can get info with `sessionInfo()`

####Don't: save output
- Avoid saving data analysis output (tables, files, images, etc) maybe
  temporarily
- Finding a file in your prodject directory that we don't know where it came
  from, is not reproducible.
- Save the data + code that generated the output, rather than the output.
- Intermediate files are ok as long as there's documentation about its
  creation.

####Do: set your seed
- random number generators generate pseudo-random number based on an initial
  seed.
 - In R you can use the `set.seed()` function to set the seed and to specify
   the random number generator to use.
- Setting the seed allows for the stream of random numbers to be exactly
  reproducible.
- Whenever you generate random number for a non-trivial purpose, always set the
  seed.

####DO: think about the entire pipeline
- Data analysis is a lengthly process, not just tables/figures/reports
- Raw data -> processed data -> analysis -> report
- How you got the end is just as important as the end itself.
- The more of the data analysis pipeline you can make reproducible, the better
  for everyone.

##Summary: Checklist
- Are we doing good science?
- Was any part of this analysis done by hand?
 - If so, are those parts *precisely* documented?
 - Does the documentation match reality?
- Have we taught a computer tro do as much as possible?
- Are we using a version control system?
- Have we documented our software environment?
- Have we saved any output that we cannot reconstruct from original data
  + code?
- How far back in the analysis pipeline can we go before our results are no
  longer reproducible?

#Evidence-based data analysis.
Replication
- Focuses on the validity of the scientific claim
- "Is this claim true?"
- The ultimate standard for strengthening scientific evidence
- New investigators, data, analytical methods, laboratories, instruments, etc.
- Particularly important in studies that can impact broad policy or regulatory
  decisions.

Reproducibility
- Focuses on the validity of the data analysis
- "Can we trust this analysis?"
- Arguably a minimum standard for any scientific study
- New investigators ,same data, same methods
- important whenm replications is impossible

The result?
- Even basic analysis are difficult to describe
- Heavy computational requirements are thrust upon people without adequate
  training in statistics and computing.
- Errors are more easily introduced into long analysis pipelines
- Knowledge transfer is inhibited
- Results are difficult to replicate or reproduce
- Complicated analyses cannot be trusted.

What problems does reproducibility solve?
- transparency
- data availability
- software/methods availability
- Improved transfer of knowledge.

What we do not get
- Validity/Correctness of the analysis.

An analysis can eb reproducible and still be wrong. We want to know "can we
trust this analysis?" Does requiring reproducibility deter bad analysis? No.

Problems with reproducibility
- The premise of reproducible research is that with data/code available, people
  can check each other and the whole system is self-correcting
- Address the most "downstream" aspect of the research process
  - post-publication
- Assumes everyone plays by the same rules and wants to achieve the same goals
  (i.e. scientific discovery.)
