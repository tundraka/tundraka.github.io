#Subsetting and sorting
```R
X[(X$var1 <= 3 & X$var3 >= 10),] # AND
X[(X$var1 <= 3 | X$var3 >= 10),] # OR
X[which($X$var2 > 8),] # which gets an index of rows where the condition is
                       # met, which doesn't include NAs
```

####Sorting
```R
# this only sorts the var1
sort(X$var1, decreasing=T, na.last=T)
# this sorts the row based on var
sort[order(X$var1, X$var2),] #sort first var1, var3

#with plry
library(plyr)
arrange(X, var1)
arrange(X, desc(var1))
```

####Adding rows
```R
X$newVar <- rnorm(5)
Y <- cbind(X, rnorm(5)) # add column
Y <- rbind(Y, rnorm(5)) # add column
```

#Summarizing data
In some sense, this is similar to what we do in exploratory data analysis,
however one thing that I noticed is that if we know something about the data,
for example, there's a column for zip codes, heights, we can make assumptions
like, there wouldn't be any negative number, in the case of zip codes no
decimal numbers, etc. These function can help assert that such assumptions are
true.

####Overview
```R
head(data, n=10) # last 10
tail(data) # last 6
summary(data) # column
str(data)
```

####Quantiles of quantitative data
```R
quantile(data$var, probs=c(0.5, 0.75, 0.9))
```

####Table
```R
# if there's any missing values, a new column will appear. By default table
# doesn't give you the amount of missing values that's why we add the useNa
attribute.
table(data, useNa="ifany")
```

####Check for missing data
```R
sum(is.na(data$column)) # total of missing data in the specified column.
any(is.na(data$column)) # true if there's missing data.
all(data$column > 0) # checks that *all* elements are greated than zero
                     # T when the condition is met.

#combining with colSums
colSums(is.na(data)) # total of na in the different columns
# expect that all sumColums are 0, meaning there are no missing values.
all(colSums(is.na(data)) == 0)
```

####Selecting values with specific characteristics
```R
# select rows that have a zip code in a specified set
table(data[data$zip %in% c('12345', '12346'),])
#or just display the data
data[data$zip %in% c('12345', '12346'),]
```

####Cross tabs
Summaries/cross tabs across several variables.
```R
xt <- xtabs(Freq ~ Gender + Admit, data=DF)
xt <- xtabs(breaks ~ ., data = warpbreaks)
ftable(xt) # easier to see presentation
```

####Size of data set
```R
d <- rnorm(1e5)
object.size(d) # 800040 bytes
print(object.size(d), units='Mb') # 0.8 Mb
```

#Creating new variables
The data set by itself won't provide all the information we need and we may need
to create those, normally by adding columns to the data set we are working
with. Commons variables to create include.

- Missingness indicators
- "Cutting up" quantitative variables
- Applying transforms

####Creating sequences
```R
seq(1, 10, by=2) # 1 3 5 7 9
seq(1, 10, length=3) # 1.0 5.5 10.0
x <- c(1, 3, 6, 8, 4, 5)
seq(along=x) # a sequence with the sequencial indices to access x
             # 1, 2, 3, 4, 5, 6
```

####Creating categorical variables
```R
data$group <- cut(data$var1, breaks=quantile(data$var1))
table(data$group)
table(data$group, dat$var1)

# another way using a library
library(Hmisc)
restData$zipGroups = cut2(restData$zipCode, g=4)
table(restData$zipGroups)
```

####Creating factors
```R
yesno <- sample(c('yes', 'no'), size=10, replace=T)
# by default the factor 1 is no because of the alphabetic precendec of 'n' over
# 'y'. With this 'yes' is 1.
yesnofac <- factor(yesno, levels=c('yes', 'no'))
relevel(yesnofac, ref='yes')
```

####Other common transforms
- `abs`: absolute value
- `sqrt`: square root
- `ceiling`: round up 3.475 -> 4
- `floor`: round down 3.475 -> 3
- `round(x, digirs=n)`: round using `n` decimals `(3.475, digits=2)` -> 3.48
- `signif(x, digits=n)': `3.475, digits=2` -> 3.5
- `cos`, `sin`, etc.
- `log` natural logarithm
- `log2`, `log10`
- `exp` exponential

####Binary variables
```R
data$less <- ifelse(data$field < 0, T, F) # kinda silly since you can just
                                          # assign the value of the expression
```

#Reshaping data
Tidy data:
- Each variable forms a column
- Each observation forms a row
- Each table/file stores data about one kind of observation (e.g. people/hosp)

melt: transform the data frame in a tall/skinny data frame, we need to
specify what columns are ids, meaning that they will end up as a combination
of rows. The value of `measure.var` will determine what columns should be
transformed to observation. In the following example, the columns `mpg` and
`hp` will end up as an observation with its value in a different column, the
two new colums that will be added are `variable`, `value`.

```R
library(reshape2) # to have melt and dcast
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id=c('carname', 'gear', 'cyl'), measure.vars=c('mpg',
'hp'))
```

To go the other way, we will make use of the `dcast` function. In this case we
will break the already tall/skinny dataset and move the `variable` observation
back to a variable.

```R
# variable has mpg and hp in it.
cylData <- dcast(carMelt, cyl ~ variable)
# We can even call a function
cylData <- dcast(carMelt, cyl ~ variable, mean)
```

####Averaging values
In the following example, the `tapply` function is used to calculate the mean
of the mpg by cyl. The first argument of `tapply` is the value that we want
to apply the function to, in this case the mpg column. The 2nd param is the
index, the index is the cyl since it's the value over which we want to iterate to
calculate the mean of mpg, and the last param is the function to apply to the
value.

The params are
1. The value.
2. The index
3. The function

```R
mpgByCyl <- tapply(mtcars$mpg, mtcars$cyl, mean)
mpgByCyl <- with(mtcars, tapply(mpg, cyl, mean))
```

There's another method call *Split-Apply-Combine*.
In this case we will use the function `split`. The split function will
split a column (1st param) into groups defined by the second param. Then we can
make use of the `lapply` function to apply a function.

```R
d <- with(mtcars, split(mpg, cyl))
mpgByCyl <- lapply(d, mean)
# going back to a vector
unlist(mpgByCyl)
# Or doing it in a single step with sapply
sapply(d, mean)
```

Yet another way includes unsing `ddply` in the `plyr` package.
```R
ddply(InsectSprays, .(spray), summarize, sum=sum(count))
```

####Other useful function
- `acast`: for casting as multi-dimensional arrays
- `arrange`: for faster reordering without using `order` commands
- `mutate`: adding new variables.

##Resources
- Andrew Jaffe's R notes: http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf
- http://statmethods.net/management/functions.html
- A tutorial from the developer of plyr - http://plyr.had.co.nz/09-user/
- A nice reshape tutorial http://www.slideshare.net/jeffreybreen/reshaping-data-in-r
- A good plyr primer - http://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/
