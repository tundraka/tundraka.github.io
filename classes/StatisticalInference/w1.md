#Introduction
Statistical inference: Go beyond the data, trying to generate new knowledge.
Trying to go beyond our data to a population to get answers. It's the only
formal inference system that we have.

> The process of generating conclusions about a population from a noisy sample.

With statistical inference, the estimators have actual things that they're
estimating. (What are estimators?)

Many different modes of statistical inference.
- bayesians
- frequentists

Frequency style thinking: think of probability like how we think of gambling.
The idea that we can repeat an experiment over and over and the percentage of
times that something happens defines that population parameter.

- Causation
- Association

#Probability
Given a random experiment (rolling a die) a probability measure is a population
quantity that summarizes the randomness
- random experiment: ?
- P = pq summarizes R
 - P = Probability measure
 - pq = population quantity
 - R = randomness

In the case of the rolling die experiment: population is an intrinsic property of
the die, not of something that is a function of a simple set of fixed rolls of
the die.

- Intrinsic: of or relating to the essential nature of a thing, inherent.

When we talk about probability is not something that is in the data that we have,
but as a conceptual thing that exists in the population that we would like to
estimate.

**Probability calculus**: rules that probability has to follow. Forms the
foundation for all thinking on probability.

Probability operates on the potential outcomes from an experiment. For
example:
- When you roll a die you could get a 1.
- Or the result can be in the set {1, 2}
- Or the roll was an even/odd number
- etc.

Probability is a function that takes any of these sets of possible outcomes and
assigns it a number between 0 and 1.

We have to have the rule that the probability that something occurs, you roll
the die and you get a number, must be 1.

The probability of the union of any two sets of outcomes that have nothing in
common (mutually exclusive) is the sum of their respective probabilities.

For example one possible outcome is that we get A={1, 2} and another outcome is
that we get B={3, 4} those two outcomes are mutually exclusive, they cannot
simultaneously occur. The probability of the union, that I get the outcome of A
or B is the sum of the probability of the A outcome plus the probability of the
B outcome `P(AUB) = P(A) + P(B)`.

These simple rules are all that is necessary to generalize to all the rules
that we think that probability should follow. Discovered by the Russian
mathematician Kolmogorov.

####Rules probability must follow
- The probability that nothing occurs is 0
- The probability that something occurs is 1
- The probability of something is 1 minus the probability that the opposite
  occurs. In the context of the die, the probability of getting an even number
  is 1 minus the probability of getting an odd number.
- The probability of at lest one of two (or more) things that are mutually
  exclusive is the sum of their respective probabilities
- If an event A implies the occurrence of event B, then the probability of
  A occurring is less than the probability that B occurs. We can think of
  A being a subset of B.
- For any two events the probability that at least one occurs is the sum of
  their probabilities minus their intersection. `P(AUB) = P(A) +
  P(B) - P(A^B)`. You cannot just add probabilities if they have a non trivial
  intersection.

#Probability mass functions
Probability calculus is useful to understand the rules that probabilities must
follow. However, we need ways to model an think about probabilities for numeric
outcomes of experiments (broadly defined). Something that is easier to work
with for numeric outcomes of experiments.

Densities and mass functions for random variables are the best starting point
for this. Probably the most famous example of a density is the so called bell
curve. We would be able to say what it means to say that the data follow a bell
curve.

When we talk about probabilities associated with the bell curve or the normal
distribution, we are talking about population quantities. It's not about what
occurs in the data. We are going with this is:use the data to estimate
properties of the population. Before we start working with the data, we need to
develop our intuition for the population quantities work

####Random variables
A **random variable** is a numerical outcome of an experiment. The random
variables that we will study come in **discrete** and **continuous**

- discrete: you can count, e.g.: number of web hits, outcome of a die. And even
  things that are not numeric like colors which can be associated with
  a number, red:1, blue:2, etc. We are going to assign a probability to every
  value that they can take
- continuous: can take any value in a continuum, we will assign probabilities
  to ranges that they can take.


####Examples of random variables
- The outcome of flipping a coin (0-1)
- The outcome of rolling a die
- Website traffic on a given day, treat as discrete, no higher bound on it. We
  would probably use the poison distribution to model that.
- BMI of a subject 4 years after a baseline measurement. A continuous random
  variable.
- Hypertension status of a subject randomly drawn from a population. We might
  give 1=hypertension, 0=no hypertension, modeled as discrete.
- Number of people who clicked on an add, similar to the website traffic.
- Intelligence quotients, normally modeled as continuous.

####Probability Mass Function (PMF)
Discrete random variables will be assigned a probability for each value they can
take. This is what the PMF will do, this function takes any value that
a discrete random variable can take and assigns the probability that it takes.

The PMF for a die roll will take 1/6 for each value that the die can produce.

There are rules that the PMF must satisfy in order for it to satisfy the basic
probability rules that were outlined before
1. it must be always be larger than 0
2. The sum of the possible values that the random variable can take has to add
  up to 1.

In the case of the die, for 2. to met, we can add the probabilities of each of
the probabilities that it can generate

X=random variable that is the result of a die roll
P=PMF associated with X
1 = P(1) + P(2) + P(3) + P(4) + P(5) + P(6)
1 = 1/6  + 1/6  + 1/6  + 1/6  + 1/6  + 1/6

PMFs that are useful:
- binomial: the canonical one for modeling the flip of a coin
- poisson: the canonical one for modeling counts

Most famous example of a PMF, the result of a coin flip, the so-called
Bernoulli distribution. This is for a fair coin:

> X=the result of a coin flop, 0=tail, 1=head. An upper case letter represents
> a potentially unrealized value of the random variable.
> p(x) = (1/2)^x(1/2)^(1-x) for x=0,1
> 
> p(0) = (1/2)^0(1/2)^(1-0) = 1 * (1/2)^1 = 1/2
> p(1) = (1/2)^1(1/2)^(1-1) = (1/2)^1 * 1 =  1/2

What about an unfair coin
> ø=theta
> p(x) = ø^x(1-ø)^(1-x) for x=0,1
> p(0) = ø^0(1-ø)^(1-0) = 1 - ø
> p(1) = ø^1(1-ø)^(1-1) = ø

This is useful for modeling the prevalence of something. For example, if we
want to model the prevalence of hypertension, we might assume that the
population or the sample that we're getting isn't unlike flips of biased coin
with the success probability theta. The issue is that we don't know theta, so
we will use our data to estimate this population proportion.

#Resources
- Brian's class Mathematical Biostatistics Boot Camp 1 has many relevant
  lectures at a more advanced level
  https://www.youtube.com/watch?v=jkUqDVtpKs4&list=PLpl-gQkQivXhk6qSyiNj51qamjAtZISJ-
